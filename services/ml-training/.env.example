# ML Training Service Configuration

# Server
PORT=8300
ENV=development

# Database
DATABASE_URL=postgresql://postgres:wave@postgres:5432/wave

# Redis (for job queue)
REDIS_URL=redis://redis:6379

# Model Storage
MODEL_CACHE_DIR=./models/cache
FINETUNED_MODEL_DIR=./models/finetuned
CHECKPOINT_DIR=./models/checkpoints

# Hugging Face
# Optional: Provide token for gated models like Llama 2
HUGGINGFACE_TOKEN=your-huggingface-token
DEFAULT_BASE_MODEL=meta-llama/Llama-2-7b-chat-hf

# Training Configuration
MAX_SEQ_LENGTH=2048
BATCH_SIZE=4
GRADIENT_ACCUMULATION_STEPS=4
LEARNING_RATE=0.0002
NUM_TRAIN_EPOCHS=3
WARMUP_STEPS=100
SAVE_STEPS=500
EVAL_STEPS=500
LOGGING_STEPS=10

# LoRA Configuration
LORA_R=16
LORA_ALPHA=32
LORA_DROPOUT=0.05
LORA_TARGET_MODULES=q_proj,v_proj

# Quantization
USE_4BIT=true
USE_8BIT=false
BNB_4BIT_COMPUTE_DTYPE=float16
BNB_4BIT_QUANT_TYPE=nf4

# Training Data
MIN_TRAINING_SAMPLES=100
VALIDATION_SPLIT=0.1

# Inference
MAX_NEW_TOKENS=500
TEMPERATURE=0.8
TOP_P=0.9
TOP_K=50

# Resource Limits
MAX_CONCURRENT_TRAININGS=2
MAX_GPU_MEMORY_GB=24

# Logging
LOG_LEVEL=INFO
